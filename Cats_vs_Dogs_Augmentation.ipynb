{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cats vs. Dogs Augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aydawudu/Computer-Vision-Intro/blob/main/Cats_vs_Dogs_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5hAMXfnnoM5"
      },
      "source": [
        "# Cats vs. Dogs Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGxCD4mGHHjG"
      },
      "source": [
        "Let's start with a model that's very effective at learning Cats v Dogs.\n",
        "\n",
        "It's similar to the previous models that you have used, but I have updated the layers definition. Note that there are now 4 convolutional layers with 32, 64, 128 and 128 convolutions respectively.\n",
        "\n",
        "Also, this will train for 100 epochs, because I want to plot the graph of loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbHyYr0URb4d",
        "outputId": "de8892f2-137b-4ddc-d160-7d8e3c631d09"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-11-16 14:28:15--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.12.240, 172.217.15.112, 172.217.164.144, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.12.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   200MB/s    in 0.3s    \n",
            "\n",
            "2021-11-16 14:28:16 (200 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "100/100 - 103s - loss: 0.7863 - acc: 0.5110 - val_loss: 0.6879 - val_acc: 0.5450 - 103s/epoch - 1s/step\n",
            "Epoch 2/20\n",
            "100/100 - 102s - loss: 0.6869 - acc: 0.5840 - val_loss: 0.6061 - val_acc: 0.6630 - 102s/epoch - 1s/step\n",
            "Epoch 3/20\n",
            "100/100 - 102s - loss: 0.6263 - acc: 0.6655 - val_loss: 0.6089 - val_acc: 0.6770 - 102s/epoch - 1s/step\n",
            "Epoch 4/20\n",
            "100/100 - 102s - loss: 0.5783 - acc: 0.7025 - val_loss: 0.5795 - val_acc: 0.6890 - 102s/epoch - 1s/step\n",
            "Epoch 5/20\n",
            "100/100 - 102s - loss: 0.5169 - acc: 0.7465 - val_loss: 0.5572 - val_acc: 0.7000 - 102s/epoch - 1s/step\n",
            "Epoch 6/20\n",
            "100/100 - 102s - loss: 0.4773 - acc: 0.7835 - val_loss: 0.5858 - val_acc: 0.7110 - 102s/epoch - 1s/step\n",
            "Epoch 7/20\n",
            "100/100 - 102s - loss: 0.4199 - acc: 0.8005 - val_loss: 0.6944 - val_acc: 0.6820 - 102s/epoch - 1s/step\n",
            "Epoch 8/20\n",
            "100/100 - 102s - loss: 0.3726 - acc: 0.8350 - val_loss: 0.6359 - val_acc: 0.7120 - 102s/epoch - 1s/step\n",
            "Epoch 9/20\n",
            "100/100 - 102s - loss: 0.3050 - acc: 0.8645 - val_loss: 0.6405 - val_acc: 0.7230 - 102s/epoch - 1s/step\n",
            "Epoch 10/20\n",
            "100/100 - 102s - loss: 0.2689 - acc: 0.8940 - val_loss: 0.6573 - val_acc: 0.7330 - 102s/epoch - 1s/step\n",
            "Epoch 11/20\n",
            "100/100 - 103s - loss: 0.2133 - acc: 0.9170 - val_loss: 0.9643 - val_acc: 0.7330 - 103s/epoch - 1s/step\n",
            "Epoch 12/20\n",
            "100/100 - 102s - loss: 0.1564 - acc: 0.9400 - val_loss: 1.0619 - val_acc: 0.7210 - 102s/epoch - 1s/step\n",
            "Epoch 13/20\n",
            "100/100 - 102s - loss: 0.1427 - acc: 0.9460 - val_loss: 0.9228 - val_acc: 0.7370 - 102s/epoch - 1s/step\n",
            "Epoch 14/20\n",
            "100/100 - 102s - loss: 0.0986 - acc: 0.9635 - val_loss: 1.1193 - val_acc: 0.7420 - 102s/epoch - 1s/step\n",
            "Epoch 15/20\n",
            "100/100 - 102s - loss: 0.1148 - acc: 0.9710 - val_loss: 1.3169 - val_acc: 0.7080 - 102s/epoch - 1s/step\n",
            "Epoch 16/20\n",
            "100/100 - 102s - loss: 0.0890 - acc: 0.9740 - val_loss: 1.4769 - val_acc: 0.7130 - 102s/epoch - 1s/step\n",
            "Epoch 17/20\n",
            "100/100 - 102s - loss: 0.0860 - acc: 0.9670 - val_loss: 1.6762 - val_acc: 0.7350 - 102s/epoch - 1s/step\n",
            "Epoch 18/20\n",
            "100/100 - 102s - loss: 0.0498 - acc: 0.9805 - val_loss: 1.5747 - val_acc: 0.7170 - 102s/epoch - 1s/step\n",
            "Epoch 19/20\n",
            "100/100 - 103s - loss: 0.1700 - acc: 0.9755 - val_loss: 1.7490 - val_acc: 0.7370 - 103s/epoch - 1s/step\n",
            "Epoch 20/20\n",
            "100/100 - 105s - loss: 0.0467 - acc: 0.9875 - val_loss: 2.2712 - val_acc: 0.7350 - 105s/epoch - 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrM9lMeXI6Ep",
        "outputId": "0f9ef399-cfc3-4776-f27e-8d1ece64c67c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
            "Traceback (most recent call last):\n",
            "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
            "KeyboardInterrupt\n",
            "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
            "Traceback (most recent call last):\n",
            "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUZfb48c+hS1G6DQiwK2ChB1AswNpQWPiKZUFU0EUEUVdWUVdU+KGsurqroqtYQCwR7CwqaMQCKhYSQAQBRQQJItKLgJSc3x/PHZiEmWSSKXdmct6v17wyc+uZm+TMnXOf+zyiqhhjjElf5fwOwBhjTHxZojfGmDRnid4YY9KcJXpjjElzluiNMSbNWaI3xpg0Z4nelIiIzBCRAbFe1k8islJEzorDdlVE/ug9Hy8id0aybCn2019EsksbZxHb7SoiebHerkm8Cn4HYOJPRHYEvawK/A7s915fo6pZkW5LVc+Lx7LpTlWHxGI7ItIY+BGoqKr7vG1nARH/Dk3ZY4m+DFDV6oHnIrISGKSqMwsvJyIVAsnDGJM+rHRThgW+movIrSLyC/CsiNQSkbdFZL2IbPaeNwha52MRGeQ9Hygin4rIg96yP4rIeaVctomIzBaR7SIyU0T+KyIvhok7khjvFpHPvO1li0jdoPmXi8gqEdkoIiOLOD6dROQXESkfNO0CEVnoPe8oIp+LyBYRWSsij4lIpTDbmiQi9wS9HuGt87OIXFVo2R4iMl9EtonIahEZHTR7tvdzi4jsEJFTAsc2aP3OIjJXRLZ6PztHemyKIiLHe+tvEZHFItIraN75IvKtt801InKzN72u9/vZIiKbROQTEbG8k2B2wM1RQG0gAxiM+5t41nvdCNgFPFbE+p2AZUBd4F/ABBGRUiz7EvAVUAcYDVxexD4jifFS4EqgPlAJCCSeE4AnvO0f4+2vASGo6pfAb8CfCm33Je/5fmC4935OAc4Eri0ibrwYunvxnA0cBxS+PvAbcAVQE+gBDBWR//PmneH9rKmq1VX180Lbrg28A4zz3tt/gHdEpE6h93DIsSkm5orAW0C2t971QJaINPcWmYArA9YATgI+9KbfBOQB9YAjgdsB63clwSzRm3xglKr+rqq7VHWjqr6uqjtVdTswFuhSxPqrVPVpVd0PPAccjfuHjnhZEWkEdADuUtU9qvopMC3cDiOM8VlV/U5VdwGvAG286RcBb6vqbFX9HbjTOwbhTAb6AYhIDeB8bxqqmquqX6jqPlVdCTwZIo5QLvHiW6Sqv+E+2ILf38eq+o2q5qvqQm9/kWwX3AfD96r6ghfXZGAp8OegZcIdm6KcDFQH7vN+Rx8Cb+MdG2AvcIKIHK6qm1V1XtD0o4EMVd2rqp+odbCVcJbozXpV3R14ISJVReRJr7SxDVcqqBlcvijkl8ATVd3pPa1ewmWPATYFTQNYHS7gCGP8Jej5zqCYjgnetpdoN4bbF+7svY+IVAb6APNUdZUXRzOvLPGLF8c/cWf3xSkQA7Cq0PvrJCIfeaWprcCQCLcb2PaqQtNWAccGvQ53bIqNWVWDPxSDt3sh7kNwlYjMEpFTvOkPAMuBbBFZISK3RfY2TCxZojeFz65uApoDnVT1cA6WCsKVY2JhLVBbRKoGTWtYxPLRxLg2eNvePuuEW1hVv8UltPMoWLYBVwJaChznxXF7aWLAlZ+CvYT7RtNQVY8Axgdtt7iz4Z9xJa1gjYA1EcRV3HYbFqqvH9iuqs5V1d64ss5U3DcFVHW7qt6kqk2BXsDfReTMKGMxJWSJ3hRWA1fz3uLVe0fFe4feGXIOMFpEKnlng38uYpVoYnwN6Ckip3kXTsdQ/P/BS8DfcB8orxaKYxuwQ0RaAEMjjOEVYKCInOB90BSOvwbuG85uEemI+4AJWI8rNTUNs+3pQDMRuVREKojIX4ATcGWWaHyJO/u/RUQqikhX3O9oivc76y8iR6jqXtwxyQcQkZ4i8kfvWsxW3HWNokplJg4s0ZvCHgYOAzYAXwDvJmi//XEXNDcC9wAv49r7h1LqGFV1MTAMl7zXAptxFwuLEqiRf6iqG4Km34xLwtuBp72YI4lhhvcePsSVNT4stMi1wBgR2Q7chXd27K27E3dN4jOvJcvJhba9EeiJ+9azEbgF6Fko7hJT1T24xH4e7rg/Dlyhqku9RS4HVnolrCG43ye4i80zgR3A58DjqvpRNLGYkhO7LmKSkYi8DCxV1bh/ozAm3dkZvUkKItJBRP4gIuW85oe9cbVeY0yU7M5YkyyOAt7AXRjNA4aq6nx/QzImPVjpxhhj0pyVbowxJs0lZemmbt262rhxY7/DMMaYlJGbm7tBVeuFmpeUib5x48bk5OT4HYYxxqQMESl8R/QBVroxxpg0Z4neGGPSnCV6Y4xJc0lZow9l79695OXlsXv37uIXNkmhSpUqNGjQgIoVK/odijFlWsok+ry8PGrUqEHjxo0JP66FSRaqysaNG8nLy6NJkyZ+h2NMmZYypZvdu3dTp04dS/IpQkSoU6eOfQMzJgmkTKIHLMmnGPt9GZMcik30ItLQG+3mW29A4L+FWKa/iCwUkW9EZI6ItA6at9KbvkBErHG8McaE8Pbb8NBDsG9f7LcdyRn9PuAmVT0BN27kMG+A5WA/Al1UtSVwN/BUofndVLWNqmZGHbEPNm7cSJs2bWjTpg1HHXUUxx577IHXe/bsKXLdnJwcbrjhhmL30blz55jE+vHHH9OzZ8+YbMsYkzhPPw2PPQYV4nDltNhEr6prAwP9egMxL6Hg+JOo6hxV3ey9/AJoEOtASyorCxo3hnLl3M+srNJvq06dOixYsIAFCxYwZMgQhg8ffuB1pUqV2FfER3BmZibjxo0rdh9z5swpfYDGmJS2dy98+CGcc058tl+iGr2INAba4oYVC+evwIyg14obGDhXRAaXNMDSyMqCwYNh1SpQdT8HD44u2Rc2cOBAhgwZQqdOnbjlllv46quvOOWUU2jbti2dO3dm2bJlQMEz7NGjR3PVVVfRtWtXmjZtWuADoHr16geW79q1KxdddBEtWrSgf//+BHoYnT59Oi1atKB9+/bccMMNJTpznzx5Mi1btuSkk07i1ltvBWD//v0MHDiQk046iZYtW/LQQw8BMG7cOE444QRatWpF3759oz9YxpgiffEF7NgRv0Qf8ZcEEakOvA7cqKrbwizTDZfoTwuafJqqrhGR+sD7IrJUVWeHWHcwMBigUaPCYyWXzMiRsHNnwWk7d7rp/fuHXqc08vLymDNnDuXLl2fbtm188sknVKhQgZkzZ3L77bfz+uuvH7LO0qVL+eijj9i+fTvNmzdn6NChh7Qznz9/PosXL+aYY47h1FNP5bPPPiMzM5NrrrmG2bNn06RJE/r16xdxnD///DO33norubm51KpVi3POOYepU6fSsGFD1qxZw6JFiwDYsmULAPfddx8//vgjlStXPjDNGBM/2dlQvjx06xaf7Ud0Ri8iFXFJPktV3wizTCvgGaC3N24lAKoaGCX+V+BNoGOo9VX1KVXNVNXMevVCdsAWsZ9+Ktn00rr44ospX748AFu3buXiiy/mpJNOYvjw4SxevDjkOj169KBy5crUrVuX+vXrs27dukOW6dixIw0aNKBcuXK0adOGlStXsnTpUpo2bXqgTXpJEv3cuXPp2rUr9erVo0KFCvTv35/Zs2fTtGlTVqxYwfXXX8+7777L4YcfDkCrVq3o378/L774IhXiUTA0xhSQnQ2dOkHNmvHZfiStbgSYACxR1f+EWaYRbnSgy1X1u6Dp1USkRuA5cA6wKBaBFyXcF4Iovygcolq1agee33nnnXTr1o1Fixbx1ltvhW0/Xrly5QPPy5cvH7K+H8kysVCrVi2+/vprunbtyvjx4xk0aBAA77zzDsOGDWPevHl06NAhbvs3xsCmTZCTE7+yDUR2Rn8qboT3P3lNJBeIyPkiMkREhnjL3IUbAu7xQs0ojwQ+FZGvga+Ad1T13Vi/icLGjoWqVQtOq1rVTY+XrVu3cuyx7hr1pEmTYr795s2bs2LFClauXAnAyy+/HPG6HTt2ZNasWWzYsIH9+/czefJkunTpwoYNG8jPz+fCCy/knnvuYd68eeTn57N69Wq6devG/fffz9atW9mxY0fM348xxvnwQ8jPj2+iL/Z7uap+ChR554uqDgIGhZi+Amh96BrxFajDjxzpyjWNGrkkH8v6fGG33HILAwYM4J577qFHjx4x3/5hhx3G448/Tvfu3alWrRodOnQIu+wHH3xAgwYHGz69+uqr3HfffXTr1g1VpUePHvTu3Zuvv/6aK6+8kvz8fADuvfde9u/fz2WXXcbWrVtRVW644QZqxuv7pDGG7Gw44ggo4l86akk5ZmxmZqYWHnhkyZIlHH/88T5FlBx27NhB9erVUVWGDRvGcccdx/Dhw/0Oq0j2ezMmPFVo0gTat4cQbTdKRERyw92rlFJdIJR1Tz/9NG3atOHEE09k69atXHPNNX6HZIyJwvffu+bf8SzbQAr1Xmlg+PDhSX8Gb4yJXHa2+xnvRG9n9MYY45PsbPjDH1z5Jp4s0RtjjA/27IGPPor/2TxYojfGGF/Eu9uDYJbojTHGB/Hu9iCYJfoIdevWjffee6/AtIcffpihQ4eGXadr164Emomef/75IfuNGT16NA8++GCR+546dSrffvvtgdd33XUXM2fOLEn4IVmXxsb4JzsbTj7ZtaGPN0v0EerXrx9TpkwpMG3KlCkR9zkzffr0Ut94VDjRjxkzhrPOOqtU2zLG+G/jxvh3exDMEn2ELrroIt55550DA42sXLmSn3/+mdNPP52hQ4eSmZnJiSeeyKhRo0Ku37hxYzZs2ADA2LFjadasGaeddtqB7ozBtZPv0KEDrVu35sILL2Tnzp3MmTOHadOmMWLECNq0acMPP/zAwIEDee211wB3F2zbtm1p2bIlV111Fb///vuB/Y0aNYp27drRsmVLli5dGvF7tS6NjYmvDz5wN0slKtGnZDv6G2+EBQtiu802beDhh8PPr127Nh07dmTGjBn07t2bKVOmcMkllyAijB07ltq1a7N//37OPPNMFi5cSKtWrUJuJzc3lylTprBgwQL27dtHu3btaN++PQB9+vTh6quvBuCOO+5gwoQJXH/99fTq1YuePXty0UUXFdjW7t27GThwIB988AHNmjXjiiuu4IknnuDGG28EoG7dusybN4/HH3+cBx98kGeeeabY42BdGhsTf9nZrqfKzASNuWdn9CUQXL4JLtu88sortGvXjrZt27J48eICZZbCPvnkEy644AKqVq3K4YcfTq9evQ7MW7RoEaeffjotW7YkKysrbFfHAcuWLaNJkyY0a9YMgAEDBjB79sGu/vv06QNA+/btD3SGVhzr0tiY+FJ1if7MM+MzbGAoKfmfWdSZdzz17t2b4cOHM2/ePHbu3En79u358ccfefDBB5k7dy61atVi4MCBYbsoLs7AgQOZOnUqrVu3ZtKkSXz88cdRxRvo7jgWXR0HujR+7733GD9+PK+88goTJ07knXfeYfbs2bz11luMHTuWb775xhK+MUX47jtYvRruuCNx+7Qz+hKoXr063bp146qrrjpwNr9t2zaqVavGEUccwbp165gxY0aR2zjjjDOYOnUqu3btYvv27bz11lsH5m3fvp2jjz6avXv3khU07mGNGjXYvn37Idtq3rw5K1euZPny5QC88MILdOnSJar3aF0aGxNfgW4Pzj47cfu0U68S6tevHxdccMGBEk7r1q1p27YtLVq0oGHDhpx66qlFrt+uXTv+8pe/0Lp1a+rXr1+gu+G7776bTp06Ua9ePTp16nQgufft25err76acePGHbgIC1ClShWeffZZLr74Yvbt20eHDh0YMmTIIfssinVpbExiZWfDccfFv9uDYNZNsYkr+70Zc9CePVC7NgwcCI89FtttWzfFxhiTBD7/HH77LXHNKgMs0RtjTIJkZ7uWNl27Jna/KZXok7HMZMKz35cxBWVnwymngNc6OWFSJtFXqVKFjRs3WvJIEarKxo0bqVKlit+hGJMUNmyA3NzEl20gglY3ItIQeB44ElDgKVV9pNAyAjwCnA/sBAaq6jxv3gAg0GL0HlV9rjSBNmjQgLy8PNavX1+a1Y0PqlSpUqBFjzFlWaDbg0Q2qwyIpHnlPuAmVZ0nIjWAXBF5X1WDb/88DzjOe3QCngA6iUhtYBSQifuQyBWRaaq6uaSBVqxYkSaJbI9kjDExlOhuD4IVW7pR1bWBs3NV3Q4sAY4ttFhv4Hl1vgBqisjRwLnA+6q6yUvu7wPdY/oOjDEmyQW6PTjrLNcHfaKVqEYvIo2BtsCXhWYdC6wOep3nTQs3PdS2B4tIjojkWHnGGJNOli6FvDx/6vNQgkQvItWB14EbVXVbrANR1adUNVNVM+vVqxfrzRtjjG/86PYgWESJXkQq4pJ8lqq+EWKRNUDDoNcNvGnhphtjTJmRnQ3NmkHjxv7sv9hE77WomQAsUdX/hFlsGnCFOCcDW1V1LfAecI6I1BKRWsA53jRjjCkTfv8dPv7Yv7INRNbq5lTgcuAbEQkM93E70AhAVccD03FNK5fjmlde6c3bJCJ3A3O99cao6qbYhW+MMcltzhzYuTPJE72qfgpIMcsoMCzMvInAxFJFZ4wxKe799/3p9iBYytwZa4wxqSg7Gzp3hho1/IvBEr0xxsTJ+vUwb56/ZRuwRG+MMXET6PbAEr0xxqSp7Gw30Ei7dv7GYYneGGPiwO9uD4JZojfGmDhYsgTWrPG/bAOW6I0xJi787vYgmCV6Y4yJg+xsaNECGjXyOxJL9MYYE3PJ0O1BMEv0xhgTY599Brt2JUfZBizRG2NMzGVnQ8WK/nZ7EMwSvTHGxFig24Pq1f2OxLFEb4wxMfTrrzB/fvLU58ESvTHGxNTMme6nJXpjjElT2dlQpw60bet3JAdZojfGmBhRdf3PJ0O3B8Es0RtjTIx8+y38/HNylW3AEr0xxsRMMnV7EMwSvTHGxEh2Nhx/PDRs6HckBRWb6EVkooj8KiKLwswfISILvMciEdkvIrW9eStF5BtvXk6sgzfGmGSxezfMmpV8ZRuI7Ix+EtA93ExVfUBV26hqG+AfwCxV3RS0SDdvfmZ0oRpjTPIKdHuQkoleVWcDm4pbztMPmBxVRMYYk4JefdV1e9Cli9+RHCpmNXoRqYo78389aLIC2SKSKyKDi1l/sIjkiEjO+vXrYxWWMcbE3csvw5NPwtVXQ7VqfkdzqFhejP0z8Fmhss1pqtoOOA8YJiJnhFtZVZ9S1UxVzaxXr14MwzLGmPiZNw+uvBJOOw0eesjvaEKLZaLvS6Gyjaqu8X7+CrwJdIzh/owxxlfr1kHv3lC3Lrz+OlSq5HdEocUk0YvIEUAX4H9B06qJSI3Ac+AcIGTLHWOMSTW//w59+sDGjTBtGtSv73dE4VUobgERmQx0BeqKSB4wCqgIoKrjvcUuALJV9begVY8E3hSRwH5eUtV3Yxe6Mcb4QxWGDYM5c1x9vk0bvyMqWrGJXlX7RbDMJFwzzOBpK4DWpQ3MGGOS1WOPwYQJcMcdcMklfkdTPLsz1hhjSuCDD2D4cFeb/3//z+9oImOJ3hiTFl5/HUaNcjctxcsPP8DFF0OLFvDCC1AuRTJoioRpjDHhLVoE/fvDmDHQoQMsXBj7fWzbBr16gYi7+FqjRuz3ES+W6I0xKW3XLujXD2rWhKws1wqmQwd4+GHIz4/NPvLz4bLLYNkydwds06ax2W6iWKI3xqS0W291Z/STJsGll7qz+XPPdXX088+HtWuj38edd8Jbb7kPjz/9KfrtJZolemNMypo+HR59FP72N+judb1Yrx7873/wxBMweza0auVKLaX18svwz3/CoEGuSWUqskRvjElJ69a5rgdatYL77is4TwSGDIHcXGjQwLWQGToUdu4s2T6Cuzf473/ddlORJXpjTMrJz4eBA90F0pdegipVQi93/PHwxRdw880wfjy0bw/z50e2j1Tp3iASluiNMSnn0Ufh3XfhwQfhxBOLXrZyZXjgATdo97Zt0KmTW6+oC7XB3Rv873/J3b1BJCzRG2NSysKFcMst0LMnXHtt5OuddZZbt2dPGDHCDRCyZs2hywV3bzBpErRtG7PQfWOJ3hiTMgJNKWvXhokTS14zr1PHlWGefho+/9zV9998s+Ayge4NRo5Mje4NImGJ3hiTMkaMgG+/heeec61rSkPEtaCZPx+aNHElmquvhh07CnZvMGZMbGP3U7GdmhljTDJ4+23X8mX48NiMy9qsmSvPjB7tWu3MmgUbNqRe9waRSKO3YoxJV2vXumaOrVvDvffGbruVKrk28h995MpCIu7iayp1bxAJO6M3xiS1QFPK335zTSkrV479Prp0gaVLXTv7dBzJ1BK9MSapPfIIZGe7O11POCF++6lWLTkH9o4FK90YY5LWggVw223u4ug11/gdTeqyRG+MSUo7d7pOyurUgWeeSd3uB5KBlW6MMUnp5pthyRJXtqlb1+9oUluxZ/QiMlFEfhWRRWHmdxWRrSKywHvcFTSvu4gsE5HlInJbLAM3xqSvadNcTf6mm+Dss/2OJvVFUrqZBHQvZplPVLWN9xgDICLlgf8C5wEnAP1EJI6XUowx6eDnn+Gqq1zXA2PH+h1Neig20avqbGBTKbbdEViuqitUdQ8wBehdiu0YY8qI/HwYMMDV5+PVlLIsitXF2FNE5GsRmSEigb7kjgVWBy2T500LSUQGi0iOiOSsX78+RmEZY1LJQw/BzJluJKcWLfyOJn3EItHPAzJUtTXwKDC1NBtR1adUNVNVM+ul4x0LxpgizZ8P//gHXHCB63vGxE7UiV5Vt6nqDu/5dKCiiNQF1gANgxZt4E0zxpgC1q51Cb5ePdezpDWljK2om1eKyFHAOlVVEemI+/DYCGwBjhORJrgE3xe4NNr9GWPSy7ZtcN55rkOxWbNcu3kTW8UmehGZDHQF6opIHjAKqAigquOBi4ChIrIP2AX0VVUF9onIdcB7QHlgoqoujsu7MMakpD17XDfBixe73inbt/c7ovRUbKJX1X7FzH8MeCzMvOnA9NKFZoxJZ/n5rhnlBx+4/uXPPdfviNKXdYFgjPHFbbdBVpbrJviKK/yOJr1ZojfGJNwjj7gBu6+91iV8E1+W6I0xCfXqq26UqAsugHHjrIVNIliiN8YkzKxZcNll0LmzK9uUL+93RGWDJXpjTEIsWuT6lW/a1HVadthhfkdUdliiN8bE3erV0L07VK0K774LtWv7HVHZYv3RG2PiassWd0PUtm3wySeQkeF3RGWPJXpjTNzs3u3KNd99587kW7f2O6KyyRK9MSYu8vPh8sth9mzX5fCf/uR3RGWX1eiNMTGn6ppQvvYaPPgg9Cvy/noTb5bojTEx9+9/uzbyN94If/+739EYS/TGmJjKyoIRI+CSS1zCtxui/GeJ3hgTMzNnwpVXQteu8PzzUM4yTFKwX4MxJiYWLHBdDjdvDm++aeO9JhNL9MaYqG3d6vquOeIImDEDatb0OyITzJpXGmOiogpDhri7Xz/5BBo08DsiU5glemNMVJ5/HqZMgbvvhlNO8TsaE4qVbowxpfb99zBsGHTpAv/4h9/RmHAs0RtjSmXPHncjVKVK8MIL1uVwMis20YvIRBH5VUQWhZnfX0QWisg3IjJHRFoHzVvpTV8gIjmxDNwY46877oDcXJgwARo29DsaU5RIzugnAd2LmP8j0EVVWwJ3A08Vmt9NVduoambpQjTGJJv333dDAV5zjWttY5JbsRdjVXW2iDQuYv6coJdfAHbN3Zg0tn69G8z7hBPgP//xOxoTiVjX6P8KzAh6rUC2iOSKyOCiVhSRwSKSIyI569evj3FYxphYUHV3vm7eDJMnu4FETPKLWfNKEemGS/SnBU0+TVXXiEh94H0RWaqqs0Otr6pP4ZV9MjMzNVZxGWNi57HH4J134JFHoFUrv6MxkYrJGb2ItAKeAXqr6sbAdFVd4/38FXgT6BiL/RljEm/hQtdZWY8ecP31fkdjSiLqRC8ijYA3gMtV9bug6dVEpEbgOXAOELLljjEmue3cCX37Qq1a8Oyz1iNlqim2dCMik4GuQF0RyQNGARUBVHU8cBdQB3hc3G9/n9fC5kjgTW9aBeAlVX03Du/BGBNnf/87LFkC2dlQr57f0ZiSiqTVTZFjw6jqIGBQiOkrABsh0pgU98Yb8OSTrmxz9tl+R2NKw+6MNcaEtXo1DBoEmZlwzz1+R2NKyxK9MSak/fvd4N579rjBvStV8jsiU1rWe6UxJqR774VZs9zF1+OO8zsaEw07ozfGHOLzz2H0aNfSZsAAv6Mx0bJEb4wpYOtWuPRS11HZ+PHWlDIdWOnGGHOAKgwdenC0qCOO8DsiEwuW6I0xBzz/vOvDxkaLSi9WujHGADZaVDqzRG+MYc8eV5e30aLSk5VujDGMHAk5OfD66zZaVDqyM3pjyrjsbHjwQRgyBPr08TsaEw+W6I0pw9atc6NFnXiijRaVzqx0Y0wZlZ8PAwe6dvMzZ8Jhh/kdkYkXS/TGlFEPPwzvvguPPw4nneR3NCaerHRjTBLYswfGjnXlE03AQJq5uXDbbfB//+dq8ya92Rm9MT5buNDVyb/+2r3+5Re4//74dT2wYwf06wf168Mzz1gXB2WBndEb45P9+11Cz8yEtWth6lR3w9IDD8Dw4fE7s7/+eli+HLKyoE6d+OzDJBc7ozfGB8uXu14h58yBCy+EJ55wQ/T16gUVK7r6+Z498NhjUC6Gp2MvvQSTJsGdd7o7YE3ZYInemARSdUl9xAh3F+qLL7o7UgPlExFXp69c2Z3t79njhvGLxZ2qK1a4enznznDXXdFvz6SOiM4VRGSiiPwqIovCzBcRGSciy0VkoYi0C5o3QES+9x7Ws7Ups/Ly4NxzXXnm9NPhm2+gf/9Da+QibtCPO++ECRPgyitdmScae/e6uny5cu6svoKd4pUpkX4pnAR0L2L+ecBx3mMw8ASAiNQGRgGdgI7AKBGpVdpgjUlFqq7/mJNOgs8+c2f0M2ZAg8oLSsoAABKNSURBVAbh1xGBMWNcL5IvvACXXeaSdWmNGgVffQVPPw0ZGaXfjomPrCxo3Nh9EDdu7F7HlKpG9AAaA4vCzHsS6Bf0ehlwNNAPeDLccuEe7du3V2PSwa+/qvbpowqqp56qunx5ybdx//1u/QsvVP3995KvP3Omqojq1VeXfN2y4sUXVTMy3HHKyHCvE7nvqlXd7zjwqFq15DEAORouf4ebcciCRSf6t4HTgl5/AGQCNwN3BE2/E7g5zDYGAzlATqNGjUp7zIxJGlOnqtavr1qpkkvW+/aVflsPPeT+W//8Z9XduyNf79dfVY8+WrVFC9UdO0q//+JEmyhTPdFGE39GRsF9Bx4ZGSV7HymR6IMfdkZvUtmWLaoDBrj/rjZtVBcujM12H3/cbbN7d9WdO4tfPj9ftUcP90GzYEHRy0aTqKJNlKmeaKONXyT0/kUifw+qiUn0VroxRlU/+EC1YUPV8uVV77ijdKWWojzzjEsAZ52l+ttvRS/7yCPuP3zcuKKXizZRRZsoUz3R+v3+AxKR6HsAMwABTga+8qbXBn4EanmPH4Haxe3LEr1JRf/8p/uPat5c9csv47ef555TLVdOtUsX1e3b3bTCZ7Rjx7oz+Z493Zl9UaJNNNEmylRPtNHGnzQ1emAysBbYC+QBfwWGAEO8+QL8F/gB+AbIDFr3KmC597gykv1ZojepZs0al1gvuKD4M+1QSlp6eOkl963h1FNVn3rq0EQholqzpur69cXv2xKtv99oAjFEe40iJmf0iXxYojepZsQId5b9ww8lX7e0iea111QrVHAfMKESTf36ke3f79JJqifaWJ2RR8sSvTFxtHmzapUq7p870RcD//e/0OsmunTgZ6ubZEi0frYaCrBEb0wcXXLJoUk2kRcD69f394w2GaR6/LFQVKIXNz+5ZGZmak5Ojt9hGFOs3buhWjU3WlNhGRmwcmXx22jcGFatKv36WVnw17/C778fnFa1Kjz1lOtiwZQNIpKrqpmh5lk3xcZQ+lvQX3ghdJIH+OmnyLYxdqxLzMGqVnXTI9G/v+sT5+ij3euMDEvypiA7ozdlXlYWDB4MO3cenBbJGfH+/XD88e5sfM+eQ+dHekYeiGHkSPfh0KiRS/KWqE1J2Bm9MUUYObJgkgf3euTIotebOhW+/x6uuSa6M3JwSX3lSvftYOVKS/ImtizRm7QQTe9/4UosRZVeVF1/8X/8Izz0kDv7z8hwvU5a6cQkG+uV2qS8wqWXVavca4gs2TZqFPpiaKNG4df5+GOYOxfGj3eDgvTvb4ndJC87ozcpr7Sll4DSXAy9/3448kg3HKAxyc4SvUl5pSm9BOvfv2SllwUL4L334G9/gypVShezMYlkid4khWhq7OFKLEWVXgorycXQf/0LatSAoUMj374xfrJEb3wXqLGvWuUucgZq7JEm+2jboZfEjz/Cyy+7ljY1a8Z++8bEgyV647toa+wlLb1E49//dhdfb7wx9ts2Jl4s0ZuYSHTzxsIS0Q59/XqYOBEuvxyOPTb22zcmXizRm6hFW3qJRY09ER591PVtM2KE35EYUzKW6A0Q3Rm5H80bE23HDnjsMejdG1q08DsaY0rGEr2J+ow80c0b/fDMM7B5M9x6q9+RGFNy1qmZibqb3GjXT3Z798If/gBNmsCsWX5HY0xo1qmZKVK0Z+SpUHqJxuTJsHq1nc2b1BVRoheR7iKyTESWi8htIeY/JCILvMd3IrIlaN7+oHnTYhm8OcjPG45SofRSWvn57gapk06C887zOxpjSqfYTs1EpDzwX+BsIA+YKyLTVPXbwDKqOjxo+euBtkGb2KWqbWIXsiks2k69xo4N3R97SbvZTYfEXtj06bB4MTz/vPsQMyYVRXJG3xFYrqorVHUPMAXoXcTy/YDJsQjORCaVbjhKNfff777Z9O3rdyTGlF4k3RQfC6wOep0HdAq1oIhkAE2AD4MmVxGRHGAfcJ+qTg2z7mBgMECjZGtAneRidcORJfaC5syBTz+Fhx+GihX9jsaY0ov1xdi+wGuquj9oWoZ3JfhS4GER+UOoFVX1KVXNVNXMevXqxTis9JYqNxylmvvvh9q1YdAgvyMxJjqRJPo1QMOg1w28aaH0pVDZRlXXeD9XAB9TsH5vPNFcTE33Vi9+WLIEpk2D666DatX8jsaY6ESS6OcCx4lIExGphEvmh7SeEZEWQC3g86BptUSksve8LnAq8G3hdcu6aG9Yshp77D3wABx2GFx/vd+RGBO9iG6YEpHzgYeB8sBEVR0rImOAHFWd5i0zGqiiqrcFrdcZeBLIx32oPKyqE4rbX1m7YSrdbzhKNXl50LSp64r40Uf9jsaYyBR1w5TdGZsEypVzZ/KFibh23Kni889h3z447bTUbop4883uAuzy5e5D2JhUUCbujFWFCy+EJ55wySaVpPrF1D17YPhw6NwZzjjD3Vz06KOwZUvx6yabzZvhySfhL3+xJG/SR9ok+q1bYdMmuPZaaNsWZs5M7P7L6sXU1auha1d3Bnzdda6/9urV4YYbXJ/tgwZBbq7fUUbuiSdcT5W33OJ3JMbEkKom3aN9+/ZaGvn5qq+/rtqkiSqo9uql+t13pdpUibz4omrVqm6fgUfVqm56SbaRkaEq4n6WZF2/zJihWqeOavXqqlOmFJyXk6M6aNDB49Khg+rEiaq//eZPrJHYuVO1fn3V7t39jsSYksNdMw2ZU31P6qEepU30Abt2qd57r0tAFSuq3nyz6pYtUW2ySBkZBZN84JGREb99+mnfPtU77nAfSi1bqi5bFn7ZzZtVx41TPf54d0xq1lS98UbVpUsTF29x9u9X/fln1XvucTF+9JHfERlTckUl+rS+GPvLL64bgGefhbp14Z574K9/dWN+xlK6XEyNxC+/wKWXwkcfwZVXusE4CpedQlGF2bNdaeSNN1zXv3/6Ewwd6gbziOedp7/95kpMP/0U+rF6tbvOAO46w6efpvbFZFM2lflWN7m5bjDnTz+F1q1dPblr15htvsw0j5w1y/X5smULPP64S/SlsW4dTJjg2vqvWgVHHeVq+YMHQ8OGBZfdv98N37d7N+zadfB5uNfr1x+ayDdsKLjNcuXc9YNGjQ4+MjLczzPOgBo1Sve+jPFTmU/04M4oX3vNjfe5ahX06eNuimnaNPptF+49EtxZbrrctBToqnfkSDcAx2uvQatW0W93/3549113lj99ujuLPvbYgkm8NC2oatQ4mLhDJfNjjoEKkfTyZEwKsUQfZNcu+M9/4N57Xflg+HCXwKI9i8vKctv56SeXTMaOTY8kv2kTXHEFvPMOXHyxG1Lv8MNjv5+VK12Lnbw8qFLFPQ477ODzcNMKv65TB2rWjH18xiQ7S/Qh/Pwz3H47PPccHHkk/POfMGBA7Ov3qeyrr1xyX7vWfTgOG2a1a2OSlSX6Isyd6+r3c+a49vfnnOPO9Ev7aNAAOnQ4+DjqqIS8jZhSdRdZb7rJlTleeQU6dvQ7KmNMUYpK9GW+Utmhg7tIe/317o7I+fPdWethh7leCytWdI9KlQ4+D35UqeLKPhUrum8Dy5dDdvbB1jaFE39mZnKXFrZtcxdGX30VevRwIyvVru13VMaYaJT5RA/w0kuuCWbgwl/gS85DD5Wuzr5jh/vAyMlx3xjmzoU33zw4/7jjCib/tm0ja6IYT/v2wZdfupY0P/zgrmHccotroWKMSW1lvnQDiWkeuXnzwcQf+JmX5+aVLw8nnujO9lu3hhYt3KNBg/gk2n37XH/rOTmu6WlODnz9tWvlcvTRMHkydOkS+/0aY+LHavTF8OuGp19+OXjGH/gACG7zXbUqNG/ukn7gZ4sW0KyZKy1FYt8+WLr0YFLPzYUFC1zrI3D90rRrB+3buw+ac891LVeMManFEn0xkuWGJ1V3w8/SpQcfy5a5nz/+ePDDKDC4SCDxB38AbNpUMKnPn18wqbdt6xJ6+/bu0ayZlWeMSQd2MbYYY8eGvuEp0b1HikD9+u5xxhkF5+3a5S70Bn8ILF3quhUIjjugWjWX1K+5pmBSt+ajxpQ9lug5eME1mW94OuwwaNnSPYLl58OaNQfP/o84wp2xW1I3xgRY6cYYY9JAmRhhKpqBP4wxJp1FlOhFpLuILBOR5SJyW4j5A0VkvYgs8B6DguYNEJHvvceAWAYfEOhUbNUqd8Fy1Sr32pK9McZEULoRkfLAd8DZQB4wF+inqt8GLTMQyFTV6wqtWxvIATIBBXKB9qq6uah9lrR0kyytZowxxi/Rlm46AstVdYWq7gGmAL0j3Pe5wPuquslL7u8D3SNcN2I//VSy6cYYU5ZEkuiPBVYHvc7zphV2oYgsFJHXRCQwfESk6yIig0UkR0Ry1q9fH0FYBzVqVLLpxhhTlsTqYuxbQGNVbYU7a3+upBtQ1adUNVNVM+vVq1eidceOPbSvGD/awRtjTDKKJNGvAYIHeGvgTTtAVTeq6u/ey2eA9pGuGwv9+7vRnDIyDt41mi6jOxljTLQiSfRzgeNEpImIVAL6AtOCFxCRo4Ne9gKWeM/fA84RkVoiUgs4x5sWc/37uwuv+fnupyV5Y4xxir0zVlX3ich1uARdHpioqotFZAyQo6rTgBtEpBewD9gEDPTW3SQid+M+LADGqOqmOLwPY4wxYdidscYYkwbKxJ2xxhhjQrNEb4wxac4SvTHGpLmkrNGLyHogRKcGEakLbCh2Kf9YfNGx+KJj8UUnmePLUNWQNyElZaKPhojkhLsgkQwsvuhYfNGx+KKT7PGFY6UbY4xJc5bojTEmzaVjon/K7wCKYfFFx+KLjsUXnWSPL6S0q9EbY4wpKB3P6I0xxgSxRG+MMWkuZRN9BOPYVhaRl735X4pI4wTG1lBEPhKRb0VksYj8LcQyXUVka9A4u3clKj5v/ytF5Btv34d0LCTOOO/4LRSRdgmMrXnQcVkgIttE5MZCyyT0+InIRBH5VUQWBU2rLSLve+Mhv+/10Bpq3biPmxwmvgdEZKn3+3tTRGqGWbfIv4U4xjdaRNYE/Q7PD7Nukf/rcYzv5aDYVorIgjDrxv34RU1VU+6B60XzB6ApUAn4Gjih0DLXAuO9532BlxMY39FAO+95DdyYu4Xj6wq87eMxXAnULWL++cAMQICTgS99/F3/grsZxLfjB5wBtAMWBU37F3Cb9/w24P4Q69UGVng/a3nPayUovnOACt7z+0PFF8nfQhzjGw3cHMHvv8j/9XjFV2j+v4G7/Dp+0T5S9Yw+knFse3NwpKvXgDNFRBIRnKquVdV53vPtuP75Qw6hmMR6A8+r8wVQs9C4A4lyJvCDqpb2TumYUNXZuC64gwX/jT0H/F+IVRMybnKo+FQ1W1X3eS+/wA3844swxy8S0YxZHbGi4vPyxiXA5FjvN1FSNdFHMhbtgWW8P/atQJ2ERBfEKxm1Bb4MMfsUEflaRGaIyIkJDQwUyBaRXBEZHGJ+xOP9xllfwv+D+Xn8AI5U1bXe81+AI0MskyzH8SrcN7RQivtbiKfrvNLSxDClr2Q4fqcD61T1+zDz/Tx+EUnVRJ8SRKQ68Dpwo6puKzR7Hq4c0Rp4FJia4PBOU9V2wHnAMBE5I8H7L5a4Ec16Aa+GmO338StA3Xf4pGyrLCIjcYMCZYVZxK+/hSeAPwBtgLW48kgy6kfRZ/NJ/7+Uqok+krFoDywjIhWAI4CNCYnO7bMiLslnqeobheer6jZV3eE9nw5UFJG6iYpPVdd4P38F3sR9RQ6WkPF+i3EeME9V1xWe4ffx86wLlLO8n7+GWMbX4ygiA4GeQH/vw+gQEfwtxIWqrlPV/aqaDzwdZr9+H78KQB/g5XDL+HX8SiJVE32x49h6rwMtHC4CPgz3hx5rXk1vArBEVf8TZpmjAtcMRKQj7neRkA8iEakmIjUCz3EX7RYVWmwacIXX+uZkYGtQmSJRwp5J+Xn8ggT/jQ0A/hdimYSNm1yYiHQHbgF6qerOMMtE8rcQr/iCr/lcEGa/kfyvx9NZwFJVzQs108/jVyJ+Xw0u7QPXKuQ73BX5kd60Mbg/aoAquK/8y4GvgKYJjO003Nf4hcAC73E+MAQY4i1zHbAY14rgC6BzAuNr6u33ay+GwPELjk+A/3rH9xsgM8G/32q4xH1E0DTfjh/uA2ctsBdXJ/4r7prPB8D3wEygtrdsJvBM0LpXeX+Hy4ErExjfclx9O/A3GGiFdgwwvai/hQTF94L3t7UQl7yPLhyf9/qQ//VExOdNnxT4mwtaNuHHL9qHdYFgjDFpLlVLN8YYYyJkid4YY9KcJXpjjElzluiNMSbNWaI3xpg0Z4neGGPSnCV6Y4xJc/8f3nORZXjXMqIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb81GvNov-Tg"
      },
      "source": [
        "The Training Accuracy is close to 100%, and the validation accuracy is in the 70%-80% range. This is a great example of overfitting -- which in short means that it can do very well with images it has seen before, but not so well with images it hasn't. Let's see if we can do better to avoid overfitting -- and one simple method is to augment the images a bit. If you think about it, most pictures of a cat are very similar -- the ears are at the top, then the eyes, then the mouth etc. Things like the distance between the eyes and ears will always be quite similar too. \n",
        "\n",
        "What if we tweak with the images to change this up a bit -- rotate the image, squash it, etc.  That's what image augementation is all about. And there's an API that makes it easy...\n",
        "\n",
        "Now take a look at the ImageGenerator. There are properties on it that you can use to augment the image. \n",
        "\n",
        "```\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "```\n",
        "These are just a few of the options available (for more, see the Keras documentation. Let's quickly go over what we just wrote:\n",
        "\n",
        "* rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
        "* width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
        "* shear_range is for randomly applying shearing transformations.\n",
        "* zoom_range is for randomly zooming inside pictures.\n",
        "* horizontal_flip is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n",
        "* fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
        "\n",
        "\n",
        "Here's some code where we've added Image Augmentation. Run it to see the impact.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrfIaBm3QT-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "0fe1ade5-e75f-45c5-e999-dc00e8632417"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir =os.path.join (base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat and dogs pictures\n",
        "train_cats_dir=os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir=os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat and dogs pictures\n",
        "validation_cats_dir=os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir=os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model=tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "\n",
        "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
        "# the image, we also rotate and do other operations\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen=ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator=train_datagen.flow_from_directory(\n",
        "    train_dir, target_size=(150,150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator=test_datagen.flow_from_directory(\n",
        "    validation_dir, target_size=(150,150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "history=model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = 100,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-16 18:32:33--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.145.128, 173.194.69.128, 173.194.79.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.145.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  18.1MB/s    in 3.6s    \n",
            "\n",
            "2021-11-16 18:32:38 (18.1 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-33685ebf3e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "qKv3t4nFBn4_",
        "outputId": "6c2c7114-88c4-4a43-9d1c-2f0c9711fe33"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8b675372579e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ]
}